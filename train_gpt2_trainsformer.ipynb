{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C38oXpGRYNe0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILD & Train GPT-decoder-only-transformer."
      ],
      "metadata": {
        "id": "5NuqoEL-iQTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n"
      ],
      "metadata": {
        "id": "f2PQlscpiYIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # How many independent training sequences we process in one forward/backward pass. B = 16\n",
        "\n",
        "# same, will need it during training. B T C\n",
        "block_size = 32 # what is the maximum context length for predictions? MOST IMPORTANT HYPERPARAMETER IN A DECODER-ONLY MODEL\n",
        "# This is the context length or maximum number of tokens the model can “see” when predicting the next token.\n",
        "# T = block_size = 32 and GPT-2 small uses 1024\n",
        "# Every attention head computes attention over T × T = 32 × 32\n",
        "# Positional embeddings table is also shaped (block_size, n_embd)\n",
        "\n",
        "\n",
        "# during training\n",
        "max_iters = 5000 # How many gradient updates we perform during training. More iterations = better convergence usually.\n",
        "eval_interval = 100 # during eval. How frequently we run evaluation on validation data (every 100 steps).\n",
        "\n",
        "# during training\n",
        "learning_rate = 1e-3 # This is extremely high for Transformer training (typical is 3e-4), but small models can handle larger LR.\n",
        "\n",
        "# optional\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Moves tensors and model to GPU if available.\n",
        "\n",
        "# during eval\n",
        "eval_iters = 200 # How many evaluation batches we run to compute test loss.\n",
        "\n",
        "# size of embedding per token, lets say there are 100 tokens, we will construct 100 x 64 token embedding table\n",
        "n_embd = 64 # Each token is embedded into a 64-dimensional vector.\n",
        "\n",
        "''' Token embedding table → (vocab_size, 64)\n",
        "\t\tPositional embedding table → (block_size, 64)\n",
        "\t\tLinear projections in Q, K, V → nn.Linear(64, head_size)\n",
        "\t\tMulti-head concat → (B, T, 64)\n",
        "\t\tFeedforward network W1 → (64, 4×64 = 256)\n",
        "\t\tFeedforward network W2 → (256, 64)\n",
        "    LayerNorm → operates over dimension 64\n",
        "  '''\n",
        "\n",
        "# number of heads, each head will be of size 64/4 = 16. Number of attention heads in each transformer block.\n",
        "n_head = 4\n",
        "\n",
        "# number of layers. Number of transformer blocks stacked sequentially\n",
        "n_layer = 4\n",
        "'''\n",
        "The block:\n",
        "Input\n",
        " → LayerNorm\n",
        " → MultiHeadAttention\n",
        " → Residual Add\n",
        " → LayerNorm\n",
        " → FeedForward\n",
        " → Residual Add\n",
        " → Output\n",
        "'''\n",
        "\n",
        "# Embedding → Block → Block → Block → Block → LayerNorm → LM Head\n",
        "#This is a 4-layer GPT-like decoder-only transformer.\n",
        "\n",
        "\n",
        "# lets see where they use dropout.\n",
        "dropout = 0.0\n",
        "\n",
        "# ------------\n",
        "'''\n",
        "Dropout used inside:\n",
        "\t•\tMulti-head attention output projection\n",
        "\t•\tFeedForward network output\n",
        "\n",
        "Since you set it to 0, dropout is disabled.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "IqspqTDrSiCp",
        "outputId": "bf153e44-7b87-4de0-9b8e-8deff0515c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDropout used inside:\\n\\t•\\tMulti-head attention output projection\\n\\t•\\tFeedForward network output\\n\\nSince you set it to 0, dropout is disabled.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You are training a GPT-2–style decoder-only transformer with:\n",
        "\t•\t4 transformer layers\n",
        "\t•\t4 attention heads per layer\n",
        "\t•\tEmbedding dim = 64\n",
        "\t•\tContext length = 32\n",
        "\t•\tBatch size = 16\n",
        "\t•\tAbout ~500k parameters\n",
        "\n",
        "### This is small enough to:\n",
        "\t•\ttrain on CPU\n",
        "\t•\tfit in 2–4GB VRAM\n",
        "\t•\tconverge in minutes\n",
        "\n",
        "#### But architecturally identical to GPT-2 (just scaled down)."
      ],
      "metadata": {
        "id": "hQY7e5H9-R9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# get random indices batches for block size\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eZeqnBLSz2v",
        "outputId": "a8829f5e-c5ca-43ce-fb5b-69da28f561e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-20 03:38:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-11-20 03:38:13 (21.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# will be used in training.\n",
        "@torch.no_grad() # prevents creation of gradients during evaluation → saves memory and speeds up the code. Use this when you only want forward passes (no backprop).\n",
        "def estimate_loss(): # defines a helper function that computes average loss estimates on both train and validation splits.\n",
        "    out = {} # empty dict to store avg losses in train and losses in val\n",
        "    model.eval() # set the PyTorch model to evaluation mode, no dropout\n",
        "    for split in ['train', 'val']: # loop twice, once to measure training loss and once for val loss\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "mEfCvGIdS4H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up a Head Class.\n",
        "# we are structuring Head class in such a way that it can be reused to compute multiple heads, where each head has its own Wq, Wk, Wv.\n",
        "# Each head object can run in parallel to learn multiple attention patterns in parallel\n",
        "\n",
        "class Head(nn.Module): # Each Head is the implementation of Attention mechanism. This head is going to be consumed in MultiHeadAttention class.\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "\n",
        "      # now each head should have their own Key, Value and Query matrices that eventually gets multiplied to Z. Where Z is the input to the transformer block.\n",
        "      # Now, these weight matrices might or might not have biases.\n",
        "      # they will internally transform the incoming input to [Input @ Weight (k/q/v)]\n",
        "        super().__init__()\n",
        "\n",
        "        # assume x: (B batches, T block_size, C n_embed)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False) # output = Z @ Wk [(B, T, n_embd)  @ (n_embd, head_size) --> (B, T, head_size)]\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False) # output = Z @ Wq [[(B, T, n_embd)  @ (n_embd, head_size) --> (B, T, head_size)] ]\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)  # output = Z @ Wv [[(B, T, n_embd)  @ (n_embd, head_size) --> (B, T, head_size)] ]\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # this creates a lower triangular matric of block_size x block_size\n",
        "        #  register_buffer registers the generated lower-triangular matrix as a \"buffer\" named 'tril' within the module\n",
        "        # causal mask: we register it so that it does not become as parameters and become part of gradients.\n",
        "        self.dropout = nn.Dropout(dropout) # why do we need a dropout in the constructor of head ?\n",
        "        # Without dropout, the model quickly memorizes patterns, especially with small datasets\n",
        "\n",
        "    '''\n",
        "    x = input to attention                      (B,T,C) or (B,T,n_embed)\n",
        "    k = key                                     (B,T,head_size)\n",
        "    q = query                                   (B,T,head_size)\n",
        "    v = value                                   (B,T,head_size)\n",
        "    wei = intermediate output of attention       (B,T,T)\n",
        "    out  = output of attention                   (B,T,head_size)\n",
        "    '''\n",
        "\n",
        "    def forward(self, x): # x: B, T, head_size or C [C == n_embed ]\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # size of k ??\n",
        "        q = self.query(x) # size of q ?\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,head_size) # i got value from x\n",
        "        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size). # last step of attention.\n",
        "        return out # [B, T, head_size] one-head-output\n"
      ],
      "metadata": {
        "id": "W6jZz5JES5xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multi-head-attention class: This class essentially does: Consumed in the transformer block. Why does this also inherit the nn module ?\n",
        "# it contains learnable layers (multiple Head objects + linear layer), it needs to register parameters in PyTorch’s graph, it needs .to(device), .parameters(), .train(), etc.\n",
        "# Every neural component → must be a module.\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size): # takes input as number of heads and head-size\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # Each head outputs ==>  h(x): (B, T, head_size)\n",
        "        # ModuleList: list list specifically for storing PyTorch submodules\n",
        "        # so that the underlying parameters get trained.\n",
        "        # Normal Python list will not register the modules, so their parameters won’t train.\n",
        "        self.proj = nn.Linear(n_embd, n_embd) # what do we do here ?\n",
        "        self.dropout = nn.Dropout(dropout) # why dropout here ?\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concat all heads.\n",
        "        out = self.dropout(self.proj(out)) # multiply with Wo the whole concatenated output. (B, T, n_embd) (n_embed, n_embed)\n",
        "        return out # output: B,T,n_embed\n"
      ],
      "metadata": {
        "id": "zpxM4WBZsy4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# having feed-forward as an class.\n",
        "# Attention handles communication between tokens.\n",
        "# The FFN then handles computation at each token separately.\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "# consumed in transformer block\n",
        "    def __init__(self, n_embd): #\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # W1: (n_embd, 4 * n_embd)\n",
        "                            # b1: (4 * n_embd) # expansion layer, giving the model more expressive capacity.\n",
        "\n",
        "            nn.ReLU(), # Adds non-linearity, so the model can learn complex functions.\n",
        "\n",
        "            nn.Linear(4 * n_embd, n_embd), # This projects back down to the original embedding size.\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): #\n",
        "        return self.net(x)\n",
        "        ''' self.net = nn.Sequential(\n",
        "    Linear1,\n",
        "    ReLU,\n",
        "    Linear2,\n",
        "    Dropout\n",
        ")\n",
        "produces the following output:\n",
        "input x gets multiplied with W1==> x1 = X @ W1 + b1  and x2 = relu(x1)  (B, T, 4*n_embd)\n",
        "input x2 gets multiplied with W2==> x3 = x2 @ W2 + b2 and x4 = relu(x3) (B, T, n_embd) and out = Dropout(x3)\n",
        "'''"
      ],
      "metadata": {
        "id": "aGabWK7iqFPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## one full Transformer decoder block, exactly like GPT-2"
      ],
      "metadata": {
        "id": "ew6AkcpN8uWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head # head size = total embeddings/number of heads.   64 / 4 = 16. Each attention head works on 16-dimensional projections of the embeddings.\n",
        "        self.sa = MultiHeadAttention(n_head, head_size) # compute attention across all heads.  output dimension of MHA is always (B, T, n_embd)\n",
        "        self.ffwd = FeedFoward(n_embd) # do ffd. output is  (B, T, n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd) # do layer norm. This normalizes each token embedding before it enters attention.\n",
        "        self.ln2 = nn.LayerNorm(n_embd) # do layer norm. This normalizes each token embedding before it enters the feedforward network.\n",
        "\n",
        "    def forward(self, x): # x goes through attention then layer norm then added into\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "Transformer Block Forward Pass\n",
        "\n",
        "Input:\n",
        "    x : (B, T, n_embd)\n",
        "\n",
        "\n",
        "The block performs two major operations:\n",
        "1) Communication across tokens (Self-Attention)\n",
        "2) Computation at each token (Feed-Forward Network)\n",
        "\n",
        "Both operations use residual connections and layer norms.\n",
        "\n",
        "===\n",
        "Step-by-step:\n",
        "\n",
        "1. a1 = ln1(x)\n",
        "   - Apply LayerNorm to x.\n",
        "   - Shape: (B, T, n_embd)\n",
        "\n",
        "2. sa_out = sa(a1)\n",
        "   - Run Multi-Head Self-Attention on the normalized input.\n",
        "   - This mixes information across tokens.\n",
        "   - Shape: (B, T, n_embd)\n",
        "\n",
        "3. x = x + sa_out\n",
        "   - First residual connection (\"Add & Norm\").\n",
        "   - Shape: (B, T, n_embd)\n",
        "\n",
        "4. a2 = ln2(x)\n",
        "   - Apply LayerNorm again before feedforward.\n",
        "   - Shape: (B, T, n_embd)\n",
        "\n",
        "5. ff_out = ffwd(a2)\n",
        "   - Run the 2-layer feedforward MLP:\n",
        "        (n_embd → 4*n_embd → n_embd)\n",
        "   - Applied independently to each token.\n",
        "   - Shape: (B, T, n_embd)\n",
        "\n",
        "6. x = x + ff_out\n",
        "   - Second residual connection.\n",
        "   - Shape: (B, T, n_embd)\n",
        "\n",
        "Output:\n",
        "    x : (B, T, n_embd)\n",
        "\n",
        "\n",
        "          ┌─────────────────┐\n",
        "x ── LN ─►│ MultiHead-Attention  │───┐\n",
        "          └─────────────────┘   │\n",
        "                                ▼\n",
        "                    Residual Add (+)\n",
        "\n",
        "          ┌─────────────────┐\n",
        "x ── LN ─►│ FeedForward MLP │───┐\n",
        "          └─────────────────┘   │\n",
        "                                ▼\n",
        "                    Residual Add (+)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "NyrPmtzUtKz9",
        "outputId": "e72952e0-21c0-45ff-ce2c-a3be70f962fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTransformer Block Forward Pass \\n\\nInput:\\n    x : (B, T, n_embd)\\n      \\n\\nThe block performs two major operations:\\n1) Communication across tokens (Self-Attention)\\n2) Computation at each token (Feed-Forward Network)\\n\\nBoth operations use residual connections and layer norms.\\n\\n===\\nStep-by-step:\\n\\n1. a1 = ln1(x)\\n   - Apply LayerNorm to x.\\n   - Shape: (B, T, n_embd)\\n\\n2. sa_out = sa(a1)\\n   - Run Multi-Head Self-Attention on the normalized input.\\n   - This mixes information across tokens.\\n   - Shape: (B, T, n_embd)\\n\\n3. x = x + sa_out\\n   - First residual connection (\"Add & Norm\").\\n   - Shape: (B, T, n_embd)\\n\\n4. a2 = ln2(x)\\n   - Apply LayerNorm again before feedforward.\\n   - Shape: (B, T, n_embd)\\n\\n5. ff_out = ffwd(a2)\\n   - Run the 2-layer feedforward MLP:\\n        (n_embd → 4*n_embd → n_embd)\\n   - Applied independently to each token.\\n   - Shape: (B, T, n_embd)\\n\\n6. x = x + ff_out\\n   - Second residual connection.\\n   - Shape: (B, T, n_embd)\\n\\nOutput:\\n    x : (B, T, n_embd)\\n\\n\\n          ┌─────────────────┐\\nx ── LN ─►│ Self-Attention  │───┐\\n          └─────────────────┘   │\\n                                ▼\\n                    Residual Add (+)\\n\\n          ┌─────────────────┐\\nx ── LN ─►│ FeedForward MLP │───┐\\n          └─────────────────┘   │\\n                                ▼\\n                    Residual Add (+)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # get embeddings of vocab x n_embed, to have a trainable embedding vector\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) #  block_size x n_embed.  this also trainable here. we dont need vocab here, because sequence/order is to be captured.\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # have 4 transformer blocks. each block maps (B,T,C) → (B,T,C), so chaining works.\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm after all transformer blocks. In PRE-LN, a final LN is REQUIRED before the output layer. to prevent prevent explosion of embeddings and improve training stability\n",
        "\n",
        "        # Projection from embedding dimension → vocabulary (input to layer: n_embd and output of this layer: vocab_size)\n",
        "        # convert the final hidden vector → next-token logits\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # at the end, i learn all global embeddings across all the vocab ? what is lm_head ? how is it this size?\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape # same as bigram lang model\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        # idx:     (B,T) integers\n",
        "        # targets: (B,T) integers\n",
        "\n",
        "\n",
        "        # convert token ids → vectors\n",
        "        # token_embedding_table(idx) returns shape (B,T,n_embd)\n",
        "        tok_emb = self.token_embedding_table(idx) # pytorch replaces each token ID in idx is by its corresponding embedding vector of size n_embd. thus B,T becomes B,T,n_embd\n",
        "\n",
        "        #  torch.arange(T) generates positions [0, 1, 2, ..., T-1].\n",
        "        # then we create embeddings for each position and self.position_embedding_table becomes a learnable matrix of size (block_size, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) # i dont understand this..\n",
        "\n",
        "\n",
        "        #tok_emb: (B,T,C)\n",
        "        #    pos_emb: (T,C) → broadcast to (B,T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C) # i hope both tok and pos are same size.\n",
        "        x = self.blocks(x) # (B,T,C) # x goes through transformer blocks\n",
        "        x = self.ln_f(x) # (B,T,C) # x goes through layer norm\n",
        "\n",
        "        # A linear classifier converting hidden vectors → logits over vocabulary.\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size) # we build logits with x, finally.\n",
        "\n",
        "        if targets is None: # when targets is none, use it as a .predict() function in the generate function?\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape # do cross-entrop loss calculation and return\n",
        "\n",
        "            # reshape logits from (B,T,C) → (B*T,C) to make it compatible for cross-entropy func\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            # cross-entropy over vocabulary\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens): # This function generates new text given an initial context (idx). Let’s break it step by step:\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            '''\n",
        "            \tassume block size = 4\n",
        "              \tCurrent sequence: [I, love, deep, learning, and, transformers] → length 6\n",
        "\t            \tLast 4 tokens: [deep, learning, and, transformers]\n",
        "\n",
        "            '''\n",
        "            idx_cond = idx[:, -block_size:] # Transformer has a context window (block_size). We take only the last block_size tokens to feed into the model. but why?\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond) # do forward pass to get logits\n",
        "\n",
        "            # logits shape: (B, T_cond, vocab_size)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C) raw scores into probabs. Logits are just raw, unnormalized scores from the final linear layer.\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1). argmax → always picks the most probable token → deterministic → can get boring repeated sequences.\n",
        "            # softmax + multinomial → picks tokens probabilistically → allows creative/random generation. Multinomial then introduces controlled randomness according to predicted probabilities.\n",
        "            # append sampled index to the running sequence.\n",
        "            # allows stochastic generation, avoids boring deterministic output\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "80u313V3tOwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel() # Creates an instance of your BigramLanguageModel class.\n",
        "m = model.to(device) # Moves the model to the specified device\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "#\ttorch.optim.AdamW is the Adam optimizer with weight decay, which helps regularize and improve generalization.\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av5M-E1wtRc9",
        "outputId": "d40027be-f0fa-4264-8efc-17780332b340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "OmKoeveuZe51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd85cc18-5719-47af-87d4-8e4bc001d9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5090, val loss 2.5058\n",
            "step 300: train loss 2.4194, val loss 2.4334\n",
            "step 400: train loss 2.3501, val loss 2.3568\n",
            "step 500: train loss 2.2963, val loss 2.3129\n",
            "step 600: train loss 2.2410, val loss 2.2501\n",
            "step 700: train loss 2.2057, val loss 2.2191\n",
            "step 800: train loss 2.1633, val loss 2.1860\n",
            "step 900: train loss 2.1242, val loss 2.1498\n",
            "step 1000: train loss 2.1027, val loss 2.1298\n",
            "step 1100: train loss 2.0692, val loss 2.1183\n",
            "step 1200: train loss 2.0386, val loss 2.0797\n",
            "step 1300: train loss 2.0276, val loss 2.0652\n",
            "step 1400: train loss 1.9925, val loss 2.0370\n",
            "step 1500: train loss 1.9702, val loss 2.0302\n",
            "step 1600: train loss 1.9645, val loss 2.0487\n",
            "step 1700: train loss 1.9421, val loss 2.0143\n",
            "step 1800: train loss 1.9091, val loss 1.9953\n",
            "step 1900: train loss 1.9085, val loss 1.9874\n",
            "step 2000: train loss 1.8861, val loss 1.9957\n",
            "step 2100: train loss 1.8731, val loss 1.9765\n",
            "step 2200: train loss 1.8607, val loss 1.9619\n",
            "step 2300: train loss 1.8552, val loss 1.9501\n",
            "step 2400: train loss 1.8410, val loss 1.9419\n",
            "step 2500: train loss 1.8160, val loss 1.9414\n",
            "step 2600: train loss 1.8272, val loss 1.9405\n",
            "step 2700: train loss 1.8118, val loss 1.9348\n",
            "step 2800: train loss 1.8041, val loss 1.9228\n",
            "step 2900: train loss 1.8056, val loss 1.9301\n",
            "step 3000: train loss 1.8001, val loss 1.9244\n",
            "step 3100: train loss 1.7672, val loss 1.9184\n",
            "step 3200: train loss 1.7548, val loss 1.9108\n",
            "step 3300: train loss 1.7586, val loss 1.9065\n",
            "step 3400: train loss 1.7571, val loss 1.8980\n",
            "step 3500: train loss 1.7399, val loss 1.8957\n",
            "step 3600: train loss 1.7283, val loss 1.8927\n",
            "step 3700: train loss 1.7292, val loss 1.8838\n",
            "step 3800: train loss 1.7209, val loss 1.8892\n",
            "step 3900: train loss 1.7222, val loss 1.8685\n",
            "step 4000: train loss 1.7162, val loss 1.8623\n",
            "step 4100: train loss 1.7164, val loss 1.8752\n",
            "step 4200: train loss 1.7036, val loss 1.8638\n",
            "step 4300: train loss 1.7016, val loss 1.8463\n",
            "step 4400: train loss 1.7053, val loss 1.8640\n",
            "step 4500: train loss 1.6885, val loss 1.8471\n",
            "step 4600: train loss 1.6917, val loss 1.8364\n",
            "step 4700: train loss 1.6839, val loss 1.8411\n",
            "step 4800: train loss 1.6672, val loss 1.8399\n",
            "step 4900: train loss 1.6708, val loss 1.8384\n",
            "step 4999: train loss 1.6627, val loss 1.8207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SN_yzzttT-G",
        "outputId": "3763bbc2-d3ef-4d1c-ba2d-6b7433054a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FlY BOLINGHARD:\n",
            "Nay, humbract; it contes too\n",
            "must encleming and the second; and say life;\n",
            "In enter all I are and those it;\n",
            "Give out of your I'll tom them nither,\n",
            "One these is news it cy rege;\n",
            "What Naying well and Burryres an fear?\n",
            "\n",
            "OXITVOHN MONFIUS:\n",
            "O is my mily.\n",
            "\n",
            "LEONTES:\n",
            "Geve worman:\n",
            "But guontt not; do spost I vour have well;\n",
            "Not and go the rivisher's become,\n",
            "And alight, upon Crame be with the On man.\n",
            "\n",
            "Roman:\n",
            "What I would and Capolicioual;\n",
            "And wife must he awour,\n",
            "Butcousins the solle with he twomment. Gefore hild you sure\n",
            "That state my not.\n",
            "\n",
            "DUKE OF YORK:\n",
            "My surnt not I have too gentle men\n",
            "Comily comport's that him; I cannot this your\n",
            "house. But as bathol! and now your and;\n",
            "Which-suppy will to coursein to shall her spersend,\n",
            "That you holk all gentled to plartes no mune in en slaicsion,\n",
            "But\n",
            "Thmal, but terruly friend\n",
            "Ristom with the rigess and wilt tentry:\n",
            "I dry that kisspy guase, we mine! crut while with up,\n",
            "I som fries that neish he pray, if,\n",
            "Thom the hre seinged fleby devir begom as goody.\n",
            "Go as thee, thou would may night.\n",
            "\n",
            "ROMEO:\n",
            "It gantle behone, thy lasbeet, him our sitive on;\n",
            "The now to be, all gokss noblambsties. joy to you would do to the woold,\n",
            "Northy will your sould in him, Andrend.\n",
            "\n",
            "LE:\n",
            "My, wense what I will betters, that them end all the sposse is seeess,\n",
            "I Tostry experirts livants you great?\n",
            "I shalk I suort set, for this glied.\n",
            "The some it, men vanty lieht. Murst; or us Volner, still;\n",
            "I wear his crumpurats there suiless Edwift a thoughanted to your ground.\n",
            "Where-be in his is\n",
            "Hard tode toble anoced me the ords,\n",
            "Wonestiful be sweet flough. were you, where 'twon enmer, 'word.\n",
            "\n",
            "POMPEY:\n",
            "Whus bot azy houth this sele yourders?\n",
            "\n",
            "POLFORD NORK:\n",
            "Yet O, sapewer, conted, so, good agion mise thy done\n",
            "on his iffather Befole wefpate,\n",
            "And hrow I teass in I knounged my spite\n",
            "but age so sucalf me with non your\n",
            "and:\n",
            "As one thums of the slive righanneds:\n",
            "Has then that with, and wein that we sterp'd hurse comison toOH!\n",
            "\n",
            "SICHAM:\n",
            "What'm, I have it:\n",
            "Twere I pear news,\n",
            "Twas wha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mrbILGPV71r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}